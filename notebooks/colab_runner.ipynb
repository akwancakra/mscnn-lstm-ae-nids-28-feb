{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSCNN-BiLSTM-AE: Two-Stage Unsupervised NIDS\n",
    "\n",
    "| | |\n",
    "|---|---|\n",
    "| **Stage 1** | Multi-Scale CNN Autoencoder (per-flow spatial features) |\n",
    "| **Stage 2** | BiLSTM Autoencoder (temporal patterns on latent sequences) |\n",
    "| **Training** | Benign CIC-IDS-2017 only |\n",
    "| **Primary eval** | CSE-CIC-IDS-2018 (unseen) |\n",
    "| **Secondary eval** | CIC-IDS-2017 all-label |\n",
    "\n",
    "Jalankan tiap cell **secara berurutan**. Setiap cell menampilkan log progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 1: Mount Drive & Clone Repo\n",
    "# ============================================================\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "\n",
    "# --- CONFIG: Sesuaikan path ini ---\n",
    "# Ganti YOUR_USERNAME dengan username GitHub kamu.\n",
    "GITHUB_REPO = 'https://github.com/YOUR_USERNAME/mscnn-lstm-ae-nids-28-feb.git'\n",
    "PROJECT_ROOT = '/content/drive/MyDrive/nids-mscnn-lstm-ae-28-feb'\n",
    "# ----------------------------------\n",
    "\n",
    "# Nama folder lokal otomatis diambil dari nama repo GitHub\n",
    "REPO_NAME = GITHUB_REPO.rstrip('/').split('/')[-1].replace('.git', '')\n",
    "REPO_DIR = f'/content/{REPO_NAME}'\n",
    "\n",
    "if not os.path.isdir(REPO_DIR):\n",
    "    !git clone {GITHUB_REPO} {REPO_DIR}\n",
    "else:\n",
    "    print(f'Repo sudah ada di {REPO_DIR}, pull latest...')\n",
    "    !cd {REPO_DIR} && git pull\n",
    "\n",
    "os.makedirs(PROJECT_ROOT, exist_ok=True)\n",
    "print(f'\\nProject root (Drive): {PROJECT_ROOT}')\n",
    "print(f'Repo dir (local):     {REPO_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 2: Install Dependencies\n",
    "# ============================================================\n",
    "!pip install -q pyyaml joblib tqdm seaborn scikit-learn scipy\n",
    "print('\\nDependencies installed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 3: Setup Path & Logging\n",
    "# ============================================================\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "REPO_DIR = '/content/mscnn-bilstm-ae-28-feb'\n",
    "if REPO_DIR not in sys.path:\n",
    "    sys.path.insert(0, REPO_DIR)\n",
    "\n",
    "os.chdir(REPO_DIR)\n",
    "print(f'Working dir: {os.getcwd()}')\n",
    "\n",
    "from src.utils import setup_logging, set_global_seed, resolve_paths, get_path, ensure_dir, save_json, save_npz\n",
    "\n",
    "setup_logging('INFO')\n",
    "logger = logging.getLogger('colab_runner')\n",
    "logger.info('Logging configured — all progress will be shown below each cell.')\n",
    "\n",
    "# Verify source tree\n",
    "src_dir = os.path.join(REPO_DIR, 'src')\n",
    "for root, dirs, files in os.walk(src_dir):\n",
    "    level = root.replace(src_dir, '').count(os.sep)\n",
    "    indent = '  ' * level\n",
    "    print(f'{indent}{os.path.basename(root)}/')\n",
    "    for f in sorted(files):\n",
    "        print(f'{indent}  {f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 4: Configuration\n",
    "# ============================================================\n",
    "PROJECT_ROOT = '/content/drive/MyDrive/nids-mscnn-lstm-ae-28-feb'\n",
    "\n",
    "CONFIG = {\n",
    "    'runtime': {\n",
    "        'colab_mode': True,\n",
    "        'drive_root': PROJECT_ROOT,\n",
    "        'random_seed': 42,\n",
    "    },\n",
    "    'paths': {\n",
    "        'data_raw_cic': 'data/raw/CIC-IDS2017',\n",
    "        'data_raw_cse': 'data/raw/CSE-CIC-IDS2018',\n",
    "        'data_processed': 'data/processed',\n",
    "        'models_dir': 'models',\n",
    "        'results_dir': 'results',\n",
    "    },\n",
    "    'preprocessing': {\n",
    "        'drop_columns': ['Flow ID'],\n",
    "        'session_columns': {\n",
    "            'src_ip': ['Source IP', 'Src IP'],\n",
    "            'dst_ip': ['Destination IP', 'Dst IP'],\n",
    "            'protocol': ['Protocol'],\n",
    "            'timestamp': ['Timestamp'],\n",
    "        },\n",
    "        'label_candidates': ['Label', 'label', 'Class'],\n",
    "        'benign_label': 'BENIGN',\n",
    "        'scaler': 'robust',\n",
    "        'post_scale_clip': 5.0,\n",
    "        'fillna_strategy': 'median',\n",
    "        'feature_filter': {\n",
    "            'nzv_threshold': 1e-5,\n",
    "            'corr_threshold': 0.98,\n",
    "        },\n",
    "        'chunksize': 50000,\n",
    "    },\n",
    "    'windowing': {\n",
    "        'mode': 'auto',\n",
    "        'window_size': 5,\n",
    "        'min_session_length': 3,\n",
    "        'fallback_mode': 'per_flow',\n",
    "    },\n",
    "    'stage1': {\n",
    "        'latent_dim': 'auto',\n",
    "        'conv_filters': [32, 32, 32],\n",
    "        'conv_kernels': [1, 3, 5],\n",
    "        'reduction_filters': 64,\n",
    "        'batch_size': 256,\n",
    "        'epochs': 100,\n",
    "        'learning_rate': 0.001,\n",
    "        'clipnorm': 1.0,\n",
    "        'early_stopping_patience': 10,\n",
    "        'reduce_lr_patience': 5,\n",
    "        'reduce_lr_factor': 0.5,\n",
    "        'min_lr': 1e-6,\n",
    "    },\n",
    "    'stage2': {\n",
    "        'temporal_latent_dim': 'auto',\n",
    "        'lstm_units': 32,\n",
    "        'dropout': 0.3,\n",
    "        'batch_size': 256,\n",
    "        'epochs': 100,\n",
    "        'learning_rate': 0.001,\n",
    "        'clipnorm': 1.0,\n",
    "        'early_stopping_patience': 10,\n",
    "        'reduce_lr_patience': 5,\n",
    "        'reduce_lr_factor': 0.5,\n",
    "        'min_lr': 1e-6,\n",
    "    },\n",
    "    'scoring': {\n",
    "        'alpha': 0.5,\n",
    "        'alpha_degenerate': 0.7,\n",
    "    },\n",
    "    'threshold': {\n",
    "        'zscore_k': [1.5, 2.0, 2.5, 3.0],\n",
    "        'percentiles': [95, 97, 99, 99.5],\n",
    "        'iqr_k': [1.5, 2.0, 3.0],\n",
    "        'target_fpr': 0.05,\n",
    "    },\n",
    "    'split': {\n",
    "        'val_size': 0.2,\n",
    "        'split_by_file': True,\n",
    "    },\n",
    "}\n",
    "\n",
    "cfg = resolve_paths(CONFIG.copy())\n",
    "set_global_seed(42)\n",
    "\n",
    "models_dir = str(ensure_dir(get_path(cfg, 'models_dir')))\n",
    "results_dir = str(ensure_dir(get_path(cfg, 'results_dir')))\n",
    "processed_dir = str(ensure_dir(get_path(cfg, 'data_processed')))\n",
    "\n",
    "pp_cfg = cfg.get('preprocessing', {})\n",
    "split_cfg = cfg.get('split', {})\n",
    "window_cfg = cfg.get('windowing', {})\n",
    "scoring_cfg = cfg.get('scoring', {})\n",
    "benign_label = pp_cfg.get('benign_label', 'BENIGN')\n",
    "\n",
    "logger.info('Config loaded. Paths resolved.')\n",
    "logger.info('  models_dir:    %s', models_dir)\n",
    "logger.info('  results_dir:   %s', results_dir)\n",
    "logger.info('  processed_dir: %s', processed_dir)\n",
    "print('\\nConfig OK.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 5: Discover Datasets & Compute Shared Features\n",
    "# ============================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from src.data.loader import list_csv_files, compute_shared_features\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "cic_files = list_csv_files(get_path(cfg, 'data_raw_cic'))\n",
    "cse_files = list_csv_files(get_path(cfg, 'data_raw_cse'))\n",
    "\n",
    "print(f'CIC-IDS-2017: {len(cic_files)} CSV files')\n",
    "for f in cic_files:\n",
    "    print(f'  {f.name}')\n",
    "\n",
    "print(f'\\nCSE-CIC-IDS-2018: {len(cse_files)} CSV files')\n",
    "for f in cse_files:\n",
    "    print(f'  {f.name}')\n",
    "\n",
    "assert len(cic_files) > 0, f'No CIC CSV files found in {get_path(cfg, \"data_raw_cic\")}'\n",
    "assert len(cse_files) > 0, f'No CSE CSV files found in {get_path(cfg, \"data_raw_cse\")}'\n",
    "\n",
    "drop_columns = pp_cfg.get('drop_columns', ['Flow ID'])\n",
    "label_candidates = pp_cfg.get('label_candidates', ['Label'])\n",
    "session_cfg = pp_cfg.get('session_columns', {})\n",
    "\n",
    "shared_features, cic_label, cse_label, cse_mapper = compute_shared_features(\n",
    "    cic_files, cse_files, drop_columns, label_candidates,\n",
    ")\n",
    "\n",
    "logger.info('Shared features: %d', len(shared_features))\n",
    "logger.info('CIC label col: %s', cic_label)\n",
    "logger.info('CSE label col: %s', cse_label)\n",
    "logger.info('CSE column mapper: %d mappings', len(cse_mapper))\n",
    "logger.info('Cell completed in %.1fs', time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 6: Load Benign CIC-2017 & Split Train/Val\n",
    "# ============================================================\n",
    "from src.data.preprocessing import load_and_prepare_benign_train\n",
    "\n",
    "t0 = time.time()\n",
    "logger.info('Loading benign CIC-2017 data...')\n",
    "\n",
    "X_train_raw, X_val_raw, meta_train, meta_val = load_and_prepare_benign_train(\n",
    "    cic_files, shared_features, cic_label, benign_label, session_cfg,\n",
    "    chunksize=pp_cfg.get('chunksize', 50000),\n",
    "    val_size=split_cfg.get('val_size', 0.2),\n",
    "    split_by_file=split_cfg.get('split_by_file', True),\n",
    ")\n",
    "\n",
    "logger.info('Benign train: %d samples, %d features', len(X_train_raw), X_train_raw.shape[1])\n",
    "logger.info('Benign val:   %d samples, %d features', len(X_val_raw), X_val_raw.shape[1])\n",
    "logger.info('Meta train:   %s', list(meta_train.columns))\n",
    "logger.info('Session IDs unique (train): %d', meta_train['session_id'].nunique())\n",
    "logger.info('Cell completed in %.1fs', time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 7: Fit Preprocessing Pipeline & Transform\n",
    "# ============================================================\n",
    "from src.data.preprocessing import PreprocessingPipeline\n",
    "\n",
    "t0 = time.time()\n",
    "logger.info('Fitting preprocessing pipeline on benign train...')\n",
    "\n",
    "pipeline = PreprocessingPipeline(cfg)\n",
    "pipeline.fit(X_train_raw, shared_features)\n",
    "pipeline.save(Path(processed_dir) / 'pipeline.joblib')\n",
    "\n",
    "logger.info('Features: %d original -> %d after filtering', pipeline.n_features_original, pipeline.n_features_final)\n",
    "logger.info('2D reshape: (%d, %d)', pipeline.nx, pipeline.ny)\n",
    "logger.info('Latent dim (Stage 1 bottleneck): %d', pipeline.latent_dim)\n",
    "logger.info('Compression ratio: %.1fx', (pipeline.nx * pipeline.ny) / pipeline.latent_dim)\n",
    "logger.info('Final features: %s', pipeline.feature_names[:10])\n",
    "if len(pipeline.feature_names) > 10:\n",
    "    logger.info('  ... and %d more', len(pipeline.feature_names) - 10)\n",
    "\n",
    "logger.info('Transforming train & val...')\n",
    "X_train = pipeline.transform(X_train_raw, reshape_2d=True)\n",
    "X_val = pipeline.transform(X_val_raw, reshape_2d=True)\n",
    "\n",
    "logger.info('X_train shape: %s', X_train.shape)\n",
    "logger.info('X_val shape:   %s', X_val.shape)\n",
    "logger.info('X_train range: [%.4f, %.4f]', X_train.min(), X_train.max())\n",
    "logger.info('Pipeline saved to %s', Path(processed_dir) / 'pipeline.joblib')\n",
    "logger.info('Cell completed in %.1fs', time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 8: Domain Shift Analysis (CIC vs CSE)\n",
    "# ============================================================\n",
    "from src.data.preprocessing import load_all_labeled\n",
    "from src.data.domain_shift import ks_test_per_feature, plot_domain_shift, summarize_shift\n",
    "from src.evaluation.metrics import binary_labels\n",
    "from IPython.display import Image, display\n",
    "\n",
    "t0 = time.time()\n",
    "logger.info('Starting domain shift analysis...')\n",
    "\n",
    "X_val_flat = X_val.reshape(len(X_val), -1)[:, :pipeline.n_features_final]\n",
    "domain_shift_report = {}\n",
    "\n",
    "try:\n",
    "    logger.info('Loading CSE-2018 data for shift comparison...')\n",
    "    X_cse_raw, y_cse_all, meta_cse = load_all_labeled(\n",
    "        cse_files, pipeline.feature_names, cse_label, session_cfg,\n",
    "        column_mapper=cse_mapper, chunksize=pp_cfg.get('chunksize', 50000),\n",
    "    )\n",
    "    cse_benign_mask = binary_labels(y_cse_all, benign_label) == 0\n",
    "    logger.info('CSE-2018 total: %d, benign: %d, attack: %d',\n",
    "                len(y_cse_all), cse_benign_mask.sum(), (~cse_benign_mask).sum())\n",
    "\n",
    "    X_cse_benign_scaled = pipeline.transform(\n",
    "        X_cse_raw[cse_benign_mask].head(50000), reshape_2d=False,\n",
    "    )\n",
    "\n",
    "    logger.info('Computing KS-test per feature...')\n",
    "    ks_df = ks_test_per_feature(\n",
    "        X_val_flat[:50000], X_cse_benign_scaled[:50000],\n",
    "        pipeline.feature_names,\n",
    "    )\n",
    "\n",
    "    shift_plot = str(Path(results_dir) / 'domain_shift.png')\n",
    "    plot_domain_shift(ks_df, save_path=shift_plot)\n",
    "    domain_shift_report = summarize_shift(ks_df)\n",
    "    save_json(Path(results_dir) / 'domain_shift.json', domain_shift_report)\n",
    "    ks_df.to_csv(Path(results_dir) / 'domain_shift_features.csv', index=False)\n",
    "\n",
    "    logger.info('Domain shift summary:')\n",
    "    logger.info('  Features with significant shift (p<0.01): %d/%d',\n",
    "                domain_shift_report['n_significant_p001'], domain_shift_report['n_features'])\n",
    "    logger.info('  High shift (KS>0.3): %d', domain_shift_report['n_high_shift_ks03'])\n",
    "    logger.info('  Mean KS: %.4f, Max KS: %.4f',\n",
    "                domain_shift_report['mean_ks'], domain_shift_report['max_ks'])\n",
    "    logger.info('  Top 5 shifted features:')\n",
    "    for item in domain_shift_report['top5_shifted']:\n",
    "        logger.info('    %s: KS=%.4f', item['feature'], item['ks_statistic'])\n",
    "\n",
    "    display(Image(filename=shift_plot, width=700))\n",
    "    print('\\nTop 10 features by KS statistic:')\n",
    "    display(ks_df.head(10))\n",
    "\n",
    "except Exception as e:\n",
    "    logger.warning('Domain shift analysis failed: %s', e)\n",
    "    domain_shift_report = {'error': str(e)}\n",
    "\n",
    "logger.info('Cell completed in %.1fs', time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 9: Train Stage 1 — MSCNN-AE\n",
    "# ============================================================\n",
    "from src.training.trainer import train_stage1, extract_latent_vectors, compute_stage1_errors\n",
    "\n",
    "t0 = time.time()\n",
    "logger.info('=' * 60)\n",
    "logger.info('STAGE 1: MSCNN-AE TRAINING')\n",
    "logger.info('=' * 60)\n",
    "logger.info('Input shape: %s', X_train.shape)\n",
    "logger.info('Batch size: %d, Max epochs: %d',\n",
    "            cfg['stage1']['batch_size'], cfg['stage1']['epochs'])\n",
    "\n",
    "s1_model, s1_encoder, s1_history = train_stage1(\n",
    "    X_train, X_val, cfg, models_dir, results_dir,\n",
    ")\n",
    "\n",
    "logger.info('Stage 1 training complete:')\n",
    "logger.info('  Total params:    %d', s1_model.count_params())\n",
    "logger.info('  Epochs trained:  %d', len(s1_history.history['loss']))\n",
    "logger.info('  Final train loss: %.6f', s1_history.history['loss'][-1])\n",
    "logger.info('  Final val loss:   %.6f', s1_history.history['val_loss'][-1])\n",
    "logger.info('  Best val loss:    %.6f', min(s1_history.history['val_loss']))\n",
    "\n",
    "print('\\nModel summary:')\n",
    "s1_model.summary()\n",
    "\n",
    "# Show training curves\n",
    "curves_path = str(Path(results_dir) / 'stage1_training_curves.png')\n",
    "if os.path.exists(curves_path):\n",
    "    display(Image(filename=curves_path, width=700))\n",
    "\n",
    "logger.info('Cell completed in %.1fs', time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 10: Extract Latent Vectors & Stage 1 Errors\n",
    "# ============================================================\n",
    "t0 = time.time()\n",
    "logger.info('Extracting latent vectors from Stage 1 encoder...')\n",
    "\n",
    "latent_train = extract_latent_vectors(s1_encoder, X_train)\n",
    "latent_val = extract_latent_vectors(s1_encoder, X_val)\n",
    "\n",
    "logger.info('Latent train: %s', latent_train.shape)\n",
    "logger.info('Latent val:   %s', latent_val.shape)\n",
    "logger.info('Latent stats (train): mean=%.4f, std=%.4f, min=%.4f, max=%.4f',\n",
    "            latent_train.mean(), latent_train.std(), latent_train.min(), latent_train.max())\n",
    "\n",
    "logger.info('Computing Stage 1 reconstruction errors...')\n",
    "err_s1_train = compute_stage1_errors(s1_model, X_train)\n",
    "err_s1_val = compute_stage1_errors(s1_model, X_val)\n",
    "\n",
    "logger.info('Stage 1 error stats:')\n",
    "logger.info('  Train: mean=%.6f, std=%.6f, p95=%.6f, p99=%.6f',\n",
    "            err_s1_train.mean(), err_s1_train.std(),\n",
    "            np.percentile(err_s1_train, 95), np.percentile(err_s1_train, 99))\n",
    "logger.info('  Val:   mean=%.6f, std=%.6f, p95=%.6f, p99=%.6f',\n",
    "            err_s1_val.mean(), err_s1_val.std(),\n",
    "            np.percentile(err_s1_val, 95), np.percentile(err_s1_val, 99))\n",
    "\n",
    "save_npz(Path(processed_dir) / 'latent_train.npz', latent=latent_train, errors=err_s1_train)\n",
    "save_npz(Path(processed_dir) / 'latent_val.npz', latent=latent_val, errors=err_s1_val)\n",
    "logger.info('Latent vectors & errors saved.')\n",
    "logger.info('Cell completed in %.1fs', time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 11: Session Analysis & Windowing\n",
    "# ============================================================\n",
    "from src.data.windowing import analyze_session_lengths, create_windows, plot_session_lengths\n",
    "\n",
    "t0 = time.time()\n",
    "logger.info('=' * 60)\n",
    "logger.info('SESSION ANALYSIS & WINDOWING')\n",
    "logger.info('=' * 60)\n",
    "\n",
    "session_stats_train = analyze_session_lengths(meta_train)\n",
    "\n",
    "logger.info('Session analysis results:')\n",
    "for k, v in session_stats_train.items():\n",
    "    logger.info('  %s: %s', k, v)\n",
    "\n",
    "sess_plot = str(Path(results_dir) / 'session_lengths.png')\n",
    "plot_session_lengths(meta_train, save_path=sess_plot)\n",
    "if os.path.exists(sess_plot):\n",
    "    display(Image(filename=sess_plot, width=700))\n",
    "\n",
    "logger.info('Creating windows for train set...')\n",
    "windows_train, _, eff_W = create_windows(\n",
    "    latent_train, meta_train, session_stats_train, window_cfg,\n",
    ")\n",
    "logger.info('Creating windows for val set...')\n",
    "windows_val, _, _ = create_windows(\n",
    "    latent_val, meta_val, session_stats_train, window_cfg,\n",
    ")\n",
    "\n",
    "logger.info('Effective window size: W=%d', eff_W)\n",
    "logger.info('Windows train: %s', windows_train.shape)\n",
    "logger.info('Windows val:   %s', windows_val.shape)\n",
    "logger.info('Cell completed in %.1fs', time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 12: Train Stage 2 — BiLSTM/Dense-AE\n",
    "# ============================================================\n",
    "from src.training.trainer import train_stage2, compute_stage2_errors\n",
    "\n",
    "t0 = time.time()\n",
    "stage2_type = 'BiLSTM-AE' if eff_W > 1 else 'Dense-AE (W=1 fallback)'\n",
    "logger.info('=' * 60)\n",
    "logger.info('STAGE 2: %s TRAINING', stage2_type)\n",
    "logger.info('=' * 60)\n",
    "logger.info('Input shape: %s', windows_train.shape)\n",
    "logger.info('Batch size: %d, Max epochs: %d',\n",
    "            cfg['stage2']['batch_size'], cfg['stage2']['epochs'])\n",
    "\n",
    "s2_model, s2_encoder, s2_history = train_stage2(\n",
    "    windows_train, windows_val,\n",
    "    latent_dim=pipeline.latent_dim,\n",
    "    window_size=eff_W,\n",
    "    cfg=cfg,\n",
    "    models_dir=models_dir,\n",
    "    results_dir=results_dir,\n",
    ")\n",
    "\n",
    "logger.info('Stage 2 training complete:')\n",
    "logger.info('  Model type:      %s', stage2_type)\n",
    "logger.info('  Total params:    %d', s2_model.count_params())\n",
    "logger.info('  Epochs trained:  %d', len(s2_history.history['loss']))\n",
    "logger.info('  Final train loss: %.6f', s2_history.history['loss'][-1])\n",
    "logger.info('  Final val loss:   %.6f', s2_history.history['val_loss'][-1])\n",
    "logger.info('  Best val loss:    %.6f', min(s2_history.history['val_loss']))\n",
    "\n",
    "print('\\nModel summary:')\n",
    "s2_model.summary()\n",
    "\n",
    "curves_path = str(Path(results_dir) / 'stage2_training_curves.png')\n",
    "if os.path.exists(curves_path):\n",
    "    display(Image(filename=curves_path, width=700))\n",
    "\n",
    "logger.info('Cell completed in %.1fs', time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 13: Threshold Determination (benign val only)\n",
    "# ============================================================\n",
    "from src.training.threshold import combine_scores, compute_all_thresholds\n",
    "\n",
    "t0 = time.time()\n",
    "logger.info('=' * 60)\n",
    "logger.info('THRESHOLD DETERMINATION (benign validation only)')\n",
    "logger.info('=' * 60)\n",
    "\n",
    "err_s2_val = compute_stage2_errors(s2_model, windows_val)\n",
    "\n",
    "alpha = scoring_cfg.get('alpha', 0.5)\n",
    "\n",
    "if eff_W > 1:\n",
    "    n_flows_covered = len(err_s2_val) * eff_W\n",
    "    err_s1_val_trunc = err_s1_val[:min(n_flows_covered, len(err_s1_val))]\n",
    "    s1_per_window = err_s1_val_trunc[:len(err_s2_val) * eff_W].reshape(-1, eff_W).mean(axis=1)\n",
    "    benign_combined = combine_scores(s1_per_window, err_s2_val, alpha=alpha)\n",
    "else:\n",
    "    benign_combined = combine_scores(err_s1_val, err_s2_val, alpha=alpha)\n",
    "\n",
    "logger.info('Combined benign val scores: mean=%.6f, std=%.6f, min=%.6f, max=%.6f',\n",
    "            benign_combined.mean(), benign_combined.std(),\n",
    "            benign_combined.min(), benign_combined.max())\n",
    "\n",
    "threshold_results = compute_all_thresholds(benign_combined, cfg)\n",
    "save_json(Path(results_dir) / 'thresholds.json', threshold_results)\n",
    "\n",
    "selected_threshold = threshold_results['selected_threshold']\n",
    "\n",
    "logger.info('\\nAll threshold strategies:')\n",
    "for name, info in threshold_results['all_thresholds'].items():\n",
    "    logger.info('  %-25s threshold=%.6f  FPR_benign_val=%.4f',\n",
    "                name, info['threshold'], info['fpr_on_benign_val'])\n",
    "\n",
    "logger.info('\\n>>> SELECTED: %s = %.6f (FPR=%.4f) <<<',\n",
    "            threshold_results['selected'], selected_threshold,\n",
    "            threshold_results['selected_fpr'])\n",
    "logger.info('Cell completed in %.1fs', time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 14: Evaluate on CIC-IDS-2017 (secondary — sanity check)\n",
    "# ============================================================\n",
    "from src.main import _evaluate_dataset\n",
    "\n",
    "t0 = time.time()\n",
    "logger.info('=' * 60)\n",
    "logger.info('EVALUATION: CIC-IDS-2017 (ALL LABELS)')\n",
    "logger.info('=' * 60)\n",
    "\n",
    "cic_metrics, cic_curves = _evaluate_dataset(\n",
    "    cic_files, pipeline, s1_model, s1_encoder, s2_model,\n",
    "    cic_label, session_cfg, session_stats_train, window_cfg,\n",
    "    eff_W, alpha, selected_threshold, benign_label,\n",
    "    dataset_name='CIC-2017', results_dir=results_dir,\n",
    "    column_mapper=None, chunksize=pp_cfg.get('chunksize', 50000),\n",
    ")\n",
    "\n",
    "logger.info('CIC-2017 Results:')\n",
    "logger.info('  ROC-AUC:   %.4f', cic_metrics['roc_auc'])\n",
    "logger.info('  PR-AUC:    %.4f', cic_metrics['pr_auc'])\n",
    "logger.info('  F1:        %.4f', cic_metrics['f1'])\n",
    "logger.info('  Precision: %.4f', cic_metrics['precision'])\n",
    "logger.info('  Recall:    %.4f', cic_metrics['recall'])\n",
    "logger.info('  FPR:       %.4f', cic_metrics['fpr'])\n",
    "logger.info('  Samples: %d (benign=%d, attack=%d)',\n",
    "            cic_metrics['n_samples'], cic_metrics['n_benign'], cic_metrics['n_attacks'])\n",
    "logger.info('Cell completed in %.1fs', time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 15: Evaluate on CSE-CIC-IDS-2018 (PRIMARY — UNSEEN)\n",
    "# ============================================================\n",
    "t0 = time.time()\n",
    "logger.info('=' * 60)\n",
    "logger.info('EVALUATION: CSE-CIC-IDS-2018 (PRIMARY — UNSEEN DATA)')\n",
    "logger.info('=' * 60)\n",
    "\n",
    "cse_metrics, cse_curves = _evaluate_dataset(\n",
    "    cse_files, pipeline, s1_model, s1_encoder, s2_model,\n",
    "    cse_label, session_cfg, session_stats_train, window_cfg,\n",
    "    eff_W, alpha, selected_threshold, benign_label,\n",
    "    dataset_name='CSE-2018', results_dir=results_dir,\n",
    "    column_mapper=cse_mapper, chunksize=pp_cfg.get('chunksize', 50000),\n",
    ")\n",
    "\n",
    "logger.info('CSE-2018 Results:')\n",
    "logger.info('  ROC-AUC:   %.4f', cse_metrics['roc_auc'])\n",
    "logger.info('  PR-AUC:    %.4f', cse_metrics['pr_auc'])\n",
    "logger.info('  F1:        %.4f', cse_metrics['f1'])\n",
    "logger.info('  Precision: %.4f', cse_metrics['precision'])\n",
    "logger.info('  Recall:    %.4f', cse_metrics['recall'])\n",
    "logger.info('  FPR:       %.4f', cse_metrics['fpr'])\n",
    "logger.info('  Samples: %d (benign=%d, attack=%d)',\n",
    "            cse_metrics['n_samples'], cse_metrics['n_benign'], cse_metrics['n_attacks'])\n",
    "logger.info('Cell completed in %.1fs', time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 16: Combined Visualizations & Generalization Analysis\n",
    "# ============================================================\n",
    "from src.evaluation.visualization import plot_roc_curves, plot_pr_curves, plot_threshold_comparison\n",
    "from src.evaluation.metrics import build_threshold_comparison_table\n",
    "\n",
    "t0 = time.time()\n",
    "logger.info('Generating combined ROC & PR curves...')\n",
    "\n",
    "roc_path = str(Path(results_dir) / 'roc_curves_combined.png')\n",
    "pr_path = str(Path(results_dir) / 'pr_curves_combined.png')\n",
    "\n",
    "plot_roc_curves(\n",
    "    {'CIC-2017': cic_curves['roc'], 'CSE-2018': cse_curves['roc']},\n",
    "    save_path=roc_path,\n",
    ")\n",
    "plot_pr_curves(\n",
    "    {'CIC-2017': cic_curves['pr'], 'CSE-2018': cse_curves['pr']},\n",
    "    save_path=pr_path,\n",
    ")\n",
    "\n",
    "# Threshold comparison table on CSE-2018\n",
    "if 'y_true' in cse_curves and 'scores' in cse_curves:\n",
    "    comp_df = build_threshold_comparison_table(\n",
    "        threshold_results, cse_curves['y_true'], cse_curves['scores'],\n",
    "        dataset_name='CSE-2018',\n",
    "    )\n",
    "    comp_df.to_csv(Path(results_dir) / 'threshold_comparison.csv', index=False)\n",
    "    plot_threshold_comparison(\n",
    "        comp_df, save_path=str(Path(results_dir) / 'threshold_comparison.png'),\n",
    "    )\n",
    "\n",
    "# Generalization analysis\n",
    "logger.info('=' * 60)\n",
    "logger.info('GENERALIZATION ANALYSIS')\n",
    "logger.info('=' * 60)\n",
    "\n",
    "auc_drop = cic_metrics.get('roc_auc', 0) - cse_metrics.get('roc_auc', 0)\n",
    "f1_drop = cic_metrics.get('f1', 0) - cse_metrics.get('f1', 0)\n",
    "\n",
    "if auc_drop < 0.10:\n",
    "    verdict = 'GOOD generalization'\n",
    "elif auc_drop < 0.20:\n",
    "    verdict = 'MODERATE generalization'\n",
    "else:\n",
    "    verdict = 'POOR generalization — likely overfitting to CIC-2017'\n",
    "\n",
    "gen_analysis = {\n",
    "    'cic_roc_auc': cic_metrics.get('roc_auc', 0),\n",
    "    'cse_roc_auc': cse_metrics.get('roc_auc', 0),\n",
    "    'cic_pr_auc': cic_metrics.get('pr_auc', 0),\n",
    "    'cse_pr_auc': cse_metrics.get('pr_auc', 0),\n",
    "    'cic_f1': cic_metrics.get('f1', 0),\n",
    "    'cse_f1': cse_metrics.get('f1', 0),\n",
    "    'auc_drop': auc_drop,\n",
    "    'f1_drop': f1_drop,\n",
    "    'verdict': verdict,\n",
    "}\n",
    "\n",
    "logger.info('CIC-2017  =>  ROC-AUC=%.4f  PR-AUC=%.4f  F1=%.4f',\n",
    "            gen_analysis['cic_roc_auc'], gen_analysis['cic_pr_auc'], gen_analysis['cic_f1'])\n",
    "logger.info('CSE-2018  =>  ROC-AUC=%.4f  PR-AUC=%.4f  F1=%.4f',\n",
    "            gen_analysis['cse_roc_auc'], gen_analysis['cse_pr_auc'], gen_analysis['cse_f1'])\n",
    "logger.info('AUC drop: %.4f', auc_drop)\n",
    "logger.info('F1 drop:  %.4f', f1_drop)\n",
    "logger.info('VERDICT:  %s', verdict)\n",
    "\n",
    "# Save final report\n",
    "report = {\n",
    "    'n_shared_features': len(shared_features),\n",
    "    'n_benign_train': len(X_train_raw),\n",
    "    'n_benign_val': len(X_val_raw),\n",
    "    'n_features_original': pipeline.n_features_original,\n",
    "    'n_features_final': pipeline.n_features_final,\n",
    "    'reshape_2d': (pipeline.nx, pipeline.ny),\n",
    "    'latent_dim': pipeline.latent_dim,\n",
    "    'effective_window_size': eff_W,\n",
    "    'stage1': {\n",
    "        'total_params': s1_model.count_params(),\n",
    "        'best_val_loss': float(min(s1_history.history['val_loss'])),\n",
    "        'n_epochs': len(s1_history.history['loss']),\n",
    "    },\n",
    "    'stage2': {\n",
    "        'model_type': stage2_type,\n",
    "        'total_params': s2_model.count_params(),\n",
    "        'best_val_loss': float(min(s2_history.history['val_loss'])),\n",
    "        'n_epochs': len(s2_history.history['loss']),\n",
    "    },\n",
    "    'thresholds': threshold_results,\n",
    "    'domain_shift': domain_shift_report,\n",
    "    'session_stats': session_stats_train,\n",
    "    'cic_metrics': cic_metrics,\n",
    "    'cse_metrics': cse_metrics,\n",
    "    'generalization': gen_analysis,\n",
    "}\n",
    "save_json(Path(results_dir) / 'final_report.json', report)\n",
    "logger.info('Final report saved to %s', Path(results_dir) / 'final_report.json')\n",
    "logger.info('Cell completed in %.1fs', time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 17: Display All Generated Plots\n",
    "# ============================================================\n",
    "results_path = Path(results_dir)\n",
    "\n",
    "plots = [\n",
    "    ('Stage 1 Training Curves', 'stage1_training_curves.png'),\n",
    "    ('Stage 2 Training Curves', 'stage2_training_curves.png'),\n",
    "    ('Domain Shift (CIC vs CSE)', 'domain_shift.png'),\n",
    "    ('Session Length Distribution', 'session_lengths.png'),\n",
    "    ('ROC Curves (CIC + CSE)', 'roc_curves_combined.png'),\n",
    "    ('PR Curves (CIC + CSE)', 'pr_curves_combined.png'),\n",
    "    ('CIC-2017 Error Distribution', 'cic2017_error_dist.png'),\n",
    "    ('CSE-2018 Error Distribution', 'cse2018_error_dist.png'),\n",
    "    ('CIC-2017 Confusion Matrix', 'cic2017_cm.png'),\n",
    "    ('CSE-2018 Confusion Matrix', 'cse2018_cm.png'),\n",
    "    ('CIC-2017 Violin Plot', 'cic2017_violin.png'),\n",
    "    ('CSE-2018 Violin Plot', 'cse2018_violin.png'),\n",
    "    ('CIC-2017 Detection Rate', 'cic2017_dr.png'),\n",
    "    ('CSE-2018 Detection Rate', 'cse2018_dr.png'),\n",
    "    ('Threshold Comparison', 'threshold_comparison.png'),\n",
    "]\n",
    "\n",
    "for title, fname in plots:\n",
    "    fp = results_path / fname\n",
    "    if fp.exists():\n",
    "        print(f'\\n{\"=\" * 50}')\n",
    "        print(f'{title}')\n",
    "        print(f'{\"=\" * 50}')\n",
    "        display(Image(filename=str(fp), width=700))\n",
    "    else:\n",
    "        print(f'  [not found] {fname}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 18: Per-Attack Detection Rates\n",
    "# ============================================================\n",
    "for ds_label, ds_key in [('CIC-IDS-2017', 'cic2017'), ('CSE-CIC-IDS-2018', 'cse2018')]:\n",
    "    dr_path = results_path / f'{ds_key}_detection_rates.csv'\n",
    "    if dr_path.exists():\n",
    "        print(f'\\n{\"=\" * 50}')\n",
    "        print(f'{ds_label} — Detection Rate per Attack Type')\n",
    "        print(f'{\"=\" * 50}')\n",
    "        dr = pd.read_csv(dr_path)\n",
    "        display(dr)\n",
    "    else:\n",
    "        print(f'  [not found] {ds_key}_detection_rates.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 19: Final Summary\n",
    "# ============================================================\n",
    "print('=' * 60)\n",
    "print('FINAL RESULTS SUMMARY')\n",
    "print('=' * 60)\n",
    "\n",
    "print(f\"\\nPreprocessing:\")\n",
    "print(f\"  Features: {pipeline.n_features_original} -> {pipeline.n_features_final}\")\n",
    "print(f\"  2D reshape: ({pipeline.nx}, {pipeline.ny})\")\n",
    "print(f\"  Latent dim: {pipeline.latent_dim}\")\n",
    "print(f\"  Window size: {eff_W}\")\n",
    "\n",
    "print(f\"\\nStage 1 (MSCNN-AE):\")\n",
    "print(f\"  Params:        {s1_model.count_params():,}\")\n",
    "print(f\"  Best val loss: {min(s1_history.history['val_loss']):.6f}\")\n",
    "print(f\"  Epochs:        {len(s1_history.history['loss'])}\")\n",
    "\n",
    "print(f\"\\nStage 2 ({stage2_type}):\")\n",
    "print(f\"  Params:        {s2_model.count_params():,}\")\n",
    "print(f\"  Best val loss: {min(s2_history.history['val_loss']):.6f}\")\n",
    "print(f\"  Epochs:        {len(s2_history.history['loss'])}\")\n",
    "\n",
    "print(f\"\\nThreshold: {threshold_results['selected']} = {selected_threshold:.6f}\")\n",
    "print(f\"  FPR on benign val: {threshold_results['selected_fpr']:.4f}\")\n",
    "\n",
    "print(f\"\\n{'Metric':<15} {'CIC-2017':>10} {'CSE-2018':>10} {'Drop':>10}\")\n",
    "print(f\"{'-'*45}\")\n",
    "for metric in ['roc_auc', 'pr_auc', 'f1', 'precision', 'recall', 'fpr']:\n",
    "    c = cic_metrics.get(metric, 0)\n",
    "    s = cse_metrics.get(metric, 0)\n",
    "    d = c - s\n",
    "    print(f\"{metric:<15} {c:>10.4f} {s:>10.4f} {d:>+10.4f}\")\n",
    "\n",
    "print(f\"\\nGeneralization verdict: {verdict}\")\n",
    "print(f\"\\nAll results saved to: {results_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 20: Domain Shift Details (optional)\n",
    "# ============================================================\n",
    "shift_path = results_path / 'domain_shift_features.csv'\n",
    "if shift_path.exists():\n",
    "    shift_df = pd.read_csv(shift_path)\n",
    "    print('Top 20 features with highest domain shift:')\n",
    "    display(shift_df.head(20))\n",
    "else:\n",
    "    print('Domain shift analysis not available.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

